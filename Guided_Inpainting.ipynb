{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UhzrFp8-MV9"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gluq4fRK-eHU"
   },
   "source": [
    "source: image with masked-out object\n",
    "\n",
    "mask: binary mask where object was removed\n",
    "\n",
    "ref: object crop, resized\n",
    "\n",
    "target: original full image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yta9AQck-FAW"
   },
   "source": [
    "Download COCO 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-392MlE9Mtl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "import requests, zipfile, io\n",
    "\n",
    "def download_coco_val(root=\"coco2017\"):\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "\n",
    "    url_images = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    url_annotations = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "\n",
    "    # Download val2017 images\n",
    "    if not os.path.exists(os.path.join(root, \"val2017\")):\n",
    "        print(\"Downloading val2017...\")\n",
    "        r = requests.get(url_images)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(root)\n",
    "\n",
    "    # Download annotations (contains val annotations too)\n",
    "    if not os.path.exists(os.path.join(root, \"annotations\")):\n",
    "        print(\"Downloading annotations...\")\n",
    "        r = requests.get(url_annotations)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(root)\n",
    "\n",
    "    print(\"COCO val2017 ready at\", root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyOW1cvw-j6U",
    "outputId": "1b40e041-9a9f-46f6-8be5-e32e1e48e17f"
   },
   "outputs": [],
   "source": [
    "download_coco_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UG1rDpo8w3v"
   },
   "source": [
    "Create Inpainting Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caoyqjdGgdEq"
   },
   "outputs": [],
   "source": [
    "def get_valid_mask_and_ref(coco, anns, img_size):\n",
    "    H, W = img_size\n",
    "    min_ratio, max_ratio = 0.05, 0.5  # thresholds\n",
    "\n",
    "    for ann in anns:\n",
    "        mask = coco.annToMask(ann)\n",
    "        mask = cv2.resize(mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "        ratio = mask.sum() / (H * W)\n",
    "        if min_ratio <= ratio <= max_ratio:  # thresholds\n",
    "            return mask, ann\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKGqdbxn-O7k"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "class CocoInpaintingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, coco_root=\"coco2017\", split=\"val2017\", image_size=256, max_objects=1):\n",
    "        super().__init__()\n",
    "        ann_file = os.path.join(coco_root, \"annotations\", f\"instances_{split}.json\")\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.root = os.path.join(coco_root, split)\n",
    "        self.max_objects = max_objects\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        path = os.path.join(self.root, img_info[\"file_name\"])\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        if len(anns) == 0:\n",
    "            return self[(idx + 1) % len(self)]  # skip empty\n",
    "\n",
    "        # Pick an object\n",
    "        # ann = random.choice(anns)\n",
    "        # mask = self.coco.annToMask(ann)\n",
    "        mask, ann = get_valid_mask_and_ref(self.coco, anns, image.size[::-1])\n",
    "        if mask is None:\n",
    "          # skip image only if ALL annotations invalid\n",
    "          return self[(idx + 1) % len(self)]\n",
    "\n",
    "        mask_img = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "\n",
    "        # Create source (masked image)\n",
    "        source = image.copy()\n",
    "        source.paste((0,0,0), mask=mask_img)\n",
    "\n",
    "        # Create reference (cropped object)\n",
    "        bbox = ann[\"bbox\"]  # [x,y,w,h]\n",
    "        x,y,w,h = map(int, bbox)\n",
    "        ref = image.crop((x,y,x+w,y+h))\n",
    "\n",
    "        # Target = original image\n",
    "        target = image\n",
    "\n",
    "        # Apply transforms\n",
    "        image = self.transform(image)\n",
    "        source = self.transform(source)\n",
    "        ref = self.transform(ref.resize((target.size[0], target.size[1])))  # match size\n",
    "        mask_tensor = self.transform(mask_img)\n",
    "\n",
    "        return {\n",
    "            \"source\": source,     # masked image\n",
    "            \"mask\": mask_tensor,  # binary mask\n",
    "            \"ref\": ref,           # reference image\n",
    "            \"target\": image       # ground truth\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAiRaJ--WE0"
   },
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCsysaFM-WmE"
   },
   "outputs": [],
   "source": [
    "def get_coco_inpainting_dataloader(root=\"coco2017\", image_size=256, batch_size=8):\n",
    "    dataset = CocoInpaintingDataset(coco_root=root, split=\"val2017\", image_size=image_size)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6zryXpx-aLN",
    "outputId": "36a7d600-44ba-4297-ce85-64adfb8cb312"
   },
   "outputs": [],
   "source": [
    "dataloader = get_coco_inpainting_dataloader(image_size=128, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2dEfEmRapNr",
    "outputId": "614bb2ef-222c-449c-e7a6-2abb07570101"
   },
   "outputs": [],
   "source": [
    "test_dataloader = get_coco_inpainting_dataloader(image_size=512, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prngGhHsdS_J",
    "outputId": "a935ba78-8832-44f3-f317-3bfcbaf7db33"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch[\"source\"].shape)  # (B, 3, 256, 256)\n",
    "print(batch[\"mask\"].shape)    # (B, 1, 256, 256)\n",
    "print(batch[\"ref\"].shape)     # (B, 3, 256, 256)\n",
    "print(batch[\"target\"].shape)  # (B, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "out7flYvxsQe",
    "outputId": "122e616b-f073-46d1-f962-1e2f8ad57d6b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, (key, tensor) in enumerate(batch.items()):\n",
    "    img = tensor[0].permute(1, 2, 0).cpu().numpy()  # take first sample, convert to HWC\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set_title(key)\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSXuTVy6-KTc"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvuUmRrtSdHT"
   },
   "source": [
    "**Guided inpainting** on top of a pretrained Stable Diffusion Inpainting model, using an extra reference image as guidance (no class/text).\n",
    "\n",
    "- Start from runwayml/stable-diffusion-inpainting (SD‑1.5 inpaint; UNet has in_channels=9: 4 noisy latents + 4 masked latents + 1 mask).\n",
    "\n",
    "- Add +4 channels for the reference image latents → new in_channels=13.\n",
    "\n",
    "- Initialize the new input conv by copying the 9 pretrained channels and zero‑init the extra 4.\n",
    "\n",
    "- Keep SD’s text pathway fixed to a single null prompt embedding internally so you don’t have to deal with text at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HEnZQLVmM5e",
    "outputId": "5a9abb49-1214-4bdf-a7ea-4dce5f082899"
   },
   "outputs": [],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynij9VIdK7Jl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0YCkz2uVBTJ"
   },
   "outputs": [],
   "source": [
    "def _expand_unet_in_channels_for_ref(unet, extra_latent_ch=4):\n",
    "    \"\"\"\n",
    "    Expand inpainting UNet input from 9 → 9 + extra_latent_ch.\n",
    "    Copies the pretrained weights for the original 9 channels,\n",
    "    zero-inits the new channels.\n",
    "    \"\"\"\n",
    "    old = unet.conv_in\n",
    "    assert isinstance(old, nn.Conv2d)\n",
    "    old_in, out_c, k, p, s, d = old.in_channels, old.out_channels, old.kernel_size, old.padding, old.stride, old.dilation\n",
    "    new_in = old_in + extra_latent_ch\n",
    "    new = nn.Conv2d(new_in, out_c, kernel_size=k, stride=s, padding=p, dilation=d, bias=old.bias is not None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new.weight.zero_()\n",
    "        new.weight[:, :old_in] = old.weight.clone()\n",
    "        if old.bias is not None:\n",
    "            new.bias.copy_(old.bias)\n",
    "\n",
    "    unet.conv_in = new\n",
    "    # Update config so schedulers/savers know the new channel count\n",
    "    if hasattr(unet, \"config\"):\n",
    "        unet.config.in_channels = new_in\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukSdSyroV7W0"
   },
   "outputs": [],
   "source": [
    "def _prep_latent_mask(mask, latent_h, latent_w, device, dtype):\n",
    "    \"\"\"\n",
    "    mask: (B,1,H,W) in {0,1} where 1==region to be repainted.\n",
    "    Downsample to latent size, keep single channel.\n",
    "    \"\"\"\n",
    "    mask = F.interpolate(mask, size=(latent_h, latent_w), mode=\"nearest\")\n",
    "    return mask.to(device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcFD9y-3WLwl"
   },
   "outputs": [],
   "source": [
    "LATENT_SCALE = 0.18215  # SD convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QA93H8SWJlX"
   },
   "outputs": [],
   "source": [
    "def _to_latents(vae, images):\n",
    "    \"\"\"\n",
    "    images in [-1,1], return latents scaled by LATENT_SCALE.\n",
    "    \"\"\"\n",
    "    posterior = vae.encode(images).latent_dist\n",
    "    z = posterior.sample() * LATENT_SCALE\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pfVbfQtWd8g"
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    Guided inpainting model:\n",
    "      - Loads SD inpainting pipeline\n",
    "      - Expands UNet to accept reference latents (extra 4 channels)\n",
    "      - Hides text: uses a fixed 'null' prompt embedding internally\n",
    "      - forward() predicts noise ε given:\n",
    "          * noisy image latents\n",
    "          * masked-image latents\n",
    "          * mask (latent size, 1ch)\n",
    "          * reference-image latents\n",
    "          * timestep t\n",
    "    \"\"\"\n",
    "    def __init__(self, im_channels, model_config):\n",
    "        super().__init__()\n",
    "\n",
    "        model_id = \"runwayml/stable-diffusion-inpainting\"\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        torch_dtype = \"float16\"\n",
    "\n",
    "        pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, safety_checker=None, feature_extractor=None\n",
    "        ).to(device)\n",
    "\n",
    "        # Expand UNet to accept +4 ref-latent channels (9 → 13)\n",
    "        _expand_unet_in_channels_for_ref(pipe.unet, extra_latent_ch=4)\n",
    "\n",
    "        # Prepare a fixed null-text embedding\n",
    "        with torch.no_grad():\n",
    "            prompt = [\"\"]  # single null prompt\n",
    "            text_inputs = pipe.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=pipe.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "            null_emb = pipe.text_encoder(**text_inputs)[0]  # (1,77,768)\n",
    "            self.register_buffer(\"null_prompt_embeds\", null_emb, persistent=False)\n",
    "\n",
    "        self.pipe = pipe\n",
    "        self.unet = pipe.unet\n",
    "        self.vae = pipe.vae\n",
    "        self.scheduler = pipe.scheduler\n",
    "        self.device = device\n",
    "        self.dtype = torch_dtype\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_images_to_latents(self, images):\n",
    "        \"\"\"\n",
    "        images: (B,3,H,W) in [-1,1]\n",
    "        returns latents: (B,4,h,w)\n",
    "        \"\"\"\n",
    "        return _to_latents(self.vae, images.to(self.device, dtype=self.vae.dtype))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents_to_images(self, latents):\n",
    "        \"\"\"\n",
    "        latents: (B,4,h,w), returns images in [-1,1]\n",
    "        \"\"\"\n",
    "        latents = latents.to(self.device, dtype=self.vae.dtype) / LATENT_SCALE\n",
    "        return self.vae.decode(latents).sample\n",
    "\n",
    "    # Prep mask to latent size\n",
    "    def prepare_mask(self, mask, latent_h, latent_w):\n",
    "        return _prep_latent_mask(mask, latent_h, latent_w, self.device, self.unet.dtype)\n",
    "\n",
    "    # Noise Prediction\n",
    "    def forward(\n",
    "        self,\n",
    "        noisy_latents,            # (B,4,h,w)\n",
    "        t,                        # (B,) or scalar timestep\n",
    "        masked_image_latents,     # (B,4,h,w)\n",
    "        mask_latent,              # (B,1,h,w) in {0,1}\n",
    "        ref_latents,              # (B,4,h,w)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns ε prediction with no text conditioning (fixed null embed).\n",
    "        UNet input is concatenation along channel dim:\n",
    "            [ noisy_latents, masked_image_latents, mask_latent, ref_latents ]  -> (B, 13, h, w)\n",
    "        \"\"\"\n",
    "        # Build model input like SD inpainting + ref latents\n",
    "        latent_model_input = torch.cat([noisy_latents, masked_image_latents, mask_latent, ref_latents], dim=1)\n",
    "\n",
    "        noise_pred = self.unet(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=self.null_prompt_embeds.expand(latent_model_input.shape[0], -1, -1),\n",
    "        ).sample\n",
    "        return noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "eda3621940c342c596ecd2c07036d55f",
      "3c8e42c1c36940048eda43633ad93d60",
      "0129212eb9794b1a9d3a3eb7a897015b",
      "ac885dbd9c5545a9bc84379fc67e0b90",
      "d3ac69fcf5d2482aa029448215f2e913",
      "9e1f3c7d38394d26a97da28448fe06a7",
      "80b655c69b5e42d3a5a91e280d7aab9d",
      "6aad3ee13c0842f7a0871cafc83914a9",
      "3097715404a94f5f9c0eab3465e7761d",
      "9e2300ca15ee48f883c31605e4d550c8",
      "40d6756c35fa46259c57fd4055914b6f"
     ]
    },
    "id": "i6VkKZdDz2Es",
    "outputId": "a7378050-da17-4748-b03d-840abe8f4f33"
   },
   "outputs": [],
   "source": [
    "# Pseudocode for the trainer loop (not the full file)\n",
    "model = Unet(im_channels=3, model_config={\"pretrained_model_name_or_path\": \"runwayml/stable-diffusion-inpainting\"})\n",
    "scheduler = model.scheduler  # DDPMScheduler already configured from the pipeline\n",
    "\n",
    "# Freeze VAE & text encoder (we don't train them)\n",
    "for p in model.vae.parameters():\n",
    "    p.requires_grad = False\n",
    "if hasattr(model, \"pipe\") and hasattr(model.pipe, \"text_encoder\"):\n",
    "    for p in model.pipe.text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Optimizer: train the UNet (including the new conv_in channels we added)\n",
    "optimizer = AdamW(\n",
    "    model.unet.parameters(),\n",
    "    lr=1e-4,              # tweak as needed\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "model.unet.enable_gradient_checkpointing()\n",
    "model.unet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "model.unet.to(model.device, dtype=torch.float16)\n",
    "model.vae.to(model.device, dtype=torch.float16)\n",
    "model.unet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "-v_lUKjcu-PK",
    "outputId": "819aceb5-7e6b-453c-c723-a88a6fee7226"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.unet.train()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "global_step = 0\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"target\"].to(model.device)\n",
    "        ref_images = batch[\"ref\"].to(model.device)\n",
    "        masks = batch[\"mask\"].to(model.device)\n",
    "        source_images = batch[\"source\"].to(model.device)\n",
    "\n",
    "        # -----------------------\n",
    "        # Encode to latents\n",
    "        # -----------------------\n",
    "        with torch.no_grad():\n",
    "            latents = model.encode_images_to_latents(images)            # (B,4,h,w)\n",
    "            ref_latents = model.encode_images_to_latents(ref_images)    # (B,4,h,w)\n",
    "            masked_image_latents = model.encode_images_to_latents(source_images)\n",
    "\n",
    "            _, _, h, w = latents.shape\n",
    "            mask_latent = F.interpolate(masks, size=(h, w), mode=\"nearest\").to(dtype=latents.dtype)\n",
    "\n",
    "        # -----------------------\n",
    "        # Noise + timestep\n",
    "        # -----------------------\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps,\n",
    "                                  (latents.size(0),), device=latents.device).long()\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        noisy_latents = noisy_latents.to(model.device, dtype=model.unet.dtype)\n",
    "        timesteps = timesteps.to(model.device)\n",
    "        masked_image_latents = masked_image_latents.to(model.device, dtype=model.unet.dtype)\n",
    "        mask_latent = mask_latent.to(model.device, dtype=model.unet.dtype)\n",
    "        ref_latents = ref_latents.to(model.device, dtype=model.unet.dtype)\n",
    "\n",
    "        # -----------------------\n",
    "        # Forward\n",
    "        # -----------------------\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            noise_pred = model(\n",
    "                noisy_latents=noisy_latents,\n",
    "                t=timesteps,\n",
    "                masked_image_latents=masked_image_latents,\n",
    "                mask_latent=mask_latent,\n",
    "                ref_latents=ref_latents,\n",
    "            )\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # -----------------------\n",
    "        # Safety checks\n",
    "        # -----------------------\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"[Step {global_step}] 🚨 Loss is NaN/Inf, skipping update\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "\n",
    "        # Backward + step\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Check gradients\n",
    "        grad_nan = False\n",
    "        for n, p in model.unet.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():\n",
    "                    print(f\"[Step {global_step}] 🚨 NaN/Inf in gradient of {n}\")\n",
    "                    grad_nan = True\n",
    "                    break\n",
    "        if grad_nan:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Check weights after step\n",
    "        weight_nan = False\n",
    "        for n, p in model.unet.named_parameters():\n",
    "            if torch.isnan(p).any() or torch.isinf(p).any():\n",
    "                print(f\"[Step {global_step}] 🚨 NaN/Inf in weights of {n}\")\n",
    "                weight_nan = True\n",
    "                break\n",
    "        if weight_nan:\n",
    "            break  # stop training immediately\n",
    "\n",
    "        # -----------------------\n",
    "        # Logging\n",
    "        # -----------------------\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"[Epoch {epoch} | Step {global_step}] loss={loss.item():.6f}\")\n",
    "\n",
    "        global_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mO0AnuHN6Oi4"
   },
   "outputs": [],
   "source": [
    "print(\"UNet device:\", next(model.unet.parameters()).device)\n",
    "print(\"UNet dtype:\", next(model.unet.parameters()).dtype)\n",
    "\n",
    "print(\"noisy_latents:\", noisy_latents.device, noisy_latents.dtype)\n",
    "print(\"masked_image_latents:\", masked_image_latents.device, masked_image_latents.dtype)\n",
    "print(\"mask_latent:\", mask_latent.device, mask_latent.dtype)\n",
    "print(\"ref_latents:\", ref_latents.device, ref_latents.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrbndcsTr4rH"
   },
   "source": [
    "# Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9crw2RVwr1gI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "epoch = 0\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "save_path = f\"checkpoints/unet_epoch{epoch+1}.pt\"\n",
    "torch.save(model.unet.state_dict(), save_path)\n",
    "print(f\"Saved checkpoint to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg-AOET01xO7"
   },
   "source": [
    "Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX6X3mye1NLX",
    "outputId": "b5c55345-519a-48f8-979e-e7a588102604"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2twOo_J1zDN"
   },
   "source": [
    "Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9XZnAOU1cEN",
    "outputId": "e12ce672-c6b8-4db5-c374-260a9b7dfbd0"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/MyDrive/inpainting_unet.pt\"\n",
    "torch.save(model.unet.state_dict(), save_path)\n",
    "print(f\"UNet saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWz5gG361irv"
   },
   "source": [
    "To reload later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N62YhSbz1fUD",
    "outputId": "2beddd0c-d777-45ba-82bc-cb590703d7a5"
   },
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionInpaintPipeline\n",
    "# import torch\n",
    "\n",
    "# model_id = \"runwayml/stable-diffusion-inpainting\"\n",
    "# pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, safety_checker=None).to(\"cuda\")\n",
    "\n",
    "# # Load trained weights\n",
    "# pipe.unet.load_state_dict(torch.load(\"/content/drive/MyDrive/inpainting_unet.pt\"))\n",
    "\n",
    "\n",
    "model.unet.load_state_dict(torch.load(\"/content/drive/MyDrive/inpainting_unet.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh1pdEtQ13Z-"
   },
   "source": [
    "Save the whole pipeline (recommended if you’ll use it later for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyGcDuDL14fm",
    "outputId": "c527ed43-9721-4df1-b705-e70ebfe28234"
   },
   "outputs": [],
   "source": [
    "save_dir = \"/content/drive/MyDrive/inpainting_pipeline\"\n",
    "model.pipe.save_pretrained(save_dir)\n",
    "print(f\"Pipeline saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EofuvzML18fF"
   },
   "source": [
    "To reload later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "c23419e7c2c84b3c8a2ef08cc88bc40f",
      "fa5acc4b3f624805be304a1dd5b4a81a",
      "95047bd422fd4cbbb533e9412f565edc",
      "a069cabb97e44d9088a1ea8f3c534ed8",
      "680dc3a5bfdb43d494334c25a6cb3f03",
      "b31fd2fce677434594ea5e46f3ae4df6",
      "e73d698d9d6e44c2ad1e1a55ceb5157b",
      "fed2f631985d4dbc91723d177b9c624e",
      "3c78390520944f4bb691a7ad4d372261",
      "2525a5c620a34ecc8eacf42c6ca975e2",
      "8ae9844d8e75432b8cd96f05b5d17f39"
     ]
    },
    "id": "4M1QAZCe16Wg",
    "outputId": "8f6ffa0c-c5b1-4f15-8af9-f469c5358e9b"
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"/content/drive/MyDrive/inpainting_pipeline\",\n",
    "    safety_checker=None\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4URyruir1_NT"
   },
   "source": [
    "Save optimizer & scheduler (optional, if you want to resume training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5z-xJUoy194M"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"unet\": model.unet.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, \"/content/drive/MyDrive/inpainting_checkpoint.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hxcF8B92CaD"
   },
   "source": [
    "To reload later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJI0r7NR2BEO"
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"/content/drive/MyDrive/inpainting_checkpoint.pt\", map_location=\"cuda\")\n",
    "model.unet.load_state_dict(ckpt[\"unet\"])\n",
    "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "# Scheduler: just re-initialize from the pipeline\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "model.scheduler = DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-inpainting\", subfolder=\"scheduler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXAuG3BQr9Dg"
   },
   "source": [
    "# Test / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHyT1tvesA5t"
   },
   "outputs": [],
   "source": [
    "def show_images(images, titles=None):\n",
    "    n = len(images)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(4*n, 4))\n",
    "\n",
    "    if n == 1:\n",
    "        axs = [axs]   # wrap single Axes into a list\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu()\n",
    "            if img.ndim == 3 and img.shape[0] in [1,3]:  # C,H,W\n",
    "                img = img.permute(1,2,0)\n",
    "            img = img.numpy()\n",
    "\n",
    "        # --- cast to float32 for imshow ---\n",
    "        if img.dtype == \"float16\":\n",
    "            img = img.astype(\"float32\")\n",
    "\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis(\"off\")\n",
    "        if titles:\n",
    "            axs[i].set_title(titles[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LPk5zoEW4Ey"
   },
   "source": [
    "little test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "DzXUHqi2LBqx",
    "outputId": "73a4b595-84b7-4a6a-9645-55763ee53ee2"
   },
   "outputs": [],
   "source": [
    "z = torch.randn(1, 4, 64, 64).to(\"cuda\")  # latent size for 512x512\n",
    "out = model.decode_latents_to_images(z)\n",
    "show_images([out[0]], [\"Random Latent Decode\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "wXGCIkgvLJrm",
    "outputId": "fb20ae01-4a7d-40bf-819d-bce15627c319"
   },
   "outputs": [],
   "source": [
    "test_latents = model.encode_images_to_latents(batch[\"target\"].to(model.device))\n",
    "test_images = model.decode_latents_to_images(test_latents)\n",
    "show_images([batch[\"target\"][0], test_images[0]], [\"Original\", \"Reconstruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57lNHf1t5X7q"
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(batch, model, num_inference_steps=50):\n",
    "    model.unet.eval()\n",
    "    with torch.no_grad():\n",
    "        images = batch[\"target\"].to(model.device)\n",
    "        ref_images = batch[\"ref\"].to(model.device)\n",
    "        masks = batch[\"mask\"].to(model.device)\n",
    "        source_images = batch[\"source\"].to(model.device)\n",
    "\n",
    "        latents = model.encode_images_to_latents(images)\n",
    "        ref_latents = model.encode_images_to_latents(ref_images)\n",
    "        masked_image_latents = model.encode_images_to_latents(source_images)\n",
    "\n",
    "        _, _, h, w = latents.shape\n",
    "        mask_latent = F.interpolate(masks, size=(h, w), mode=\"nearest\").to(dtype=latents.dtype)\n",
    "\n",
    "        # --- Start from scaled noise ---\n",
    "        # model.scheduler.set_timesteps(num_inference_steps, device=model.device)\n",
    "        # noisy_latents = torch.randn_like(latents) * model.scheduler.init_noise_sigma\n",
    "        noisy_latents = masked_image_latents * (1 - mask_latent) + torch.randn_like(masked_image_latents) * mask_latent\n",
    "\n",
    "        # Example: 50 inference steps\n",
    "        num_inference_steps = 50\n",
    "        model.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        # --- Denoising loop ---\n",
    "        for t in model.scheduler.timesteps:\n",
    "          timesteps = torch.full((latents.shape[0],), t, device=model.device, dtype=torch.long)\n",
    "          with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "              noise_pred = model(\n",
    "                  noisy_latents,\n",
    "                  t=timesteps,\n",
    "                  masked_image_latents=masked_image_latents,\n",
    "                  mask_latent=mask_latent,\n",
    "                  ref_latents=ref_latents\n",
    "              )\n",
    "\n",
    "          latents = model.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # --- Decode (with scaling back) ---\n",
    "        recon = model.decode_latents_to_images(latents)\n",
    "        recon = (recon * 0.5 + 0.5).clamp(0,1).to(torch.float32)\n",
    "\n",
    "    # recon: [B, C, H_model, W_model]\n",
    "    # source_images: [B, C, H_img, W_img]\n",
    "    # mask_latent: [B, 1, h_latent, w_latent]\n",
    "\n",
    "    # 1. Upsample mask to match source_images\n",
    "    mask_resized = F.interpolate(mask_latent, size=source_images.shape[2:], mode=\"nearest\")\n",
    "\n",
    "    # 2. If mask has 1 channel but images have 3, repeat channels\n",
    "    if mask_resized.shape[1] == 1 and source_images.shape[1] > 1:\n",
    "        mask_resized = mask_resized.repeat(1, source_images.shape[1], 1, 1)\n",
    "\n",
    "    # 3. Ensure recon matches source_images size (if decoder returns slightly different size)\n",
    "    if recon.shape[2:] != source_images.shape[2:]:\n",
    "        recon = F.interpolate(recon, size=source_images.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # 4. Blend\n",
    "    final_output = recon * mask_resized + source_images * (1 - mask_resized)\n",
    "\n",
    "\n",
    "    show_images([\n",
    "        source_images[0], ref_images[0], images[0], recon[0], final_output[0]\n",
    "    ], [\"Masked Source\", \"Reference\", \"Target\", \"Reconstruction\", \"Blended Output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "yfrBy2OFsC3u",
    "outputId": "90c1e392-659c-4281-c558-14b9b5419946"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "evaluate_one_batch(batch, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGSZBo-osFvX"
   },
   "source": [
    "# Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98nHdr1nsHOS"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRQ1fbEKsJE4"
   },
   "outputs": [],
   "source": [
    "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "psnr = PeakSignalNoiseRatio().to(model.device)\n",
    "ssim = StructuralSimilarityIndexMeasure().to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score_psnr = psnr(recon, images).item()\n",
    "    score_ssim = ssim(recon, images).item()\n",
    "print(f\"PSNR: {score_psnr:.2f}, SSIM: {score_ssim:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
